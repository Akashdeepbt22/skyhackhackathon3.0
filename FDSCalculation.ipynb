{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f2d84a16-9195-4b00-b650-043fab5ef3c3",
      "cell_type": "code",
      "source": "\nimport pandas as pd\nimport numpy as np\n\nFLIGHT_FILE   = \"Flight Level Data.csv\"\nBAG_FILE      = \"Bag+Level+Data.csv\"\nPNR_FILE      = \"PNR+Flight+Level+Data.csv\"\nREMARKS_FILE  = \"PNR Remark Level Data.csv\"\nOUTPUT_FILE   = \"Flight_Difficulty_Score_final_with_rank_and_classification.csv\"\n\n\n \ndef safe_minmax_ser(s):\n    s2 = s.dropna()\n    if len(s2) == 0:\n        return 0.0, 1.0\n    return s2.min(), s2.max()\n\ndef minmax_scale(series, lo=None, hi=None):\n    s = series.copy().astype(float)\n    if lo is None or hi is None:\n        lo, hi = safe_minmax_ser(s)\n    if hi == lo:\n        return pd.Series(0.0, index=s.index)\n    return (s - lo) / (hi - lo)\n\ndef drop_duplicated_columns(df):\n    if df.columns.duplicated().any():\n        df = df.loc[:, ~df.columns.duplicated()]\n    return df\n\ndef detect_date_col(df):\n    for c in df.columns:\n        low = c.lower()\n        if 'scheduled_departure' in low or 'flight_date' in low or low=='date':\n            return c\n    # fallback to any date-like column name\n    for c in df.columns:\n        if 'date' in c.lower():\n            return c\n    return None\n\n# Loading flight-level data and using basic features \nfl = pd.read_csv(FLIGHT_FILE)\n\n# Getting scheduled and actual departure columns\nsched_col = next((c for c in fl.columns if 'scheduled_departure' in c.lower() and 'datetime' in c.lower()), \n                 next((c for c in fl.columns if 'scheduled_departure' in c.lower()), None))\nactual_col = next((c for c in fl.columns if 'actual_departure' in c.lower() and 'datetime' in c.lower()), \n                  next((c for c in fl.columns if 'actual_departure' in c.lower()), None))\n\nif sched_col is None:\n    raise KeyError(\"Could not find scheduled departure datetime column in FlightLevelData.csv\")\n\nfl[sched_col] = pd.to_datetime(fl[sched_col], errors='coerce')\nif actual_col:\n    fl[actual_col] = pd.to_datetime(fl[actual_col], errors='coerce')\n\n# flight date\nfl['flight_date'] = fl[sched_col].dt.date\n\n# departure delay in minutes\nif actual_col:\n    fl['departure_delay_min'] = (fl[actual_col] - fl[sched_col]).dt.total_seconds() / 60.0\nelse:\n    delay_col = next((c for c in fl.columns if 'dep_delay' in c.lower() or 'departure_delay' in c.lower()), None)\n    fl['departure_delay_min'] = pd.to_numeric(fl[delay_col], errors='coerce') if delay_col else np.nan\n\n# numeric ground times\nfl['scheduled_ground_time_minutes'] = pd.to_numeric(fl.get('scheduled_ground_time_minutes'), errors='coerce')\nfl['minimum_turn_minutes'] = pd.to_numeric(fl.get('minimum_turn_minutes'), errors='coerce')\n\n# ground pressure and turnaround slack\nfl['ground_pressure'] = np.where((fl['scheduled_ground_time_minutes'].notna()) & (fl['scheduled_ground_time_minutes']>0),\n                                fl['minimum_turn_minutes'] / fl['scheduled_ground_time_minutes'],\n                                np.nan)\nfl['turnaround_slack'] = fl['scheduled_ground_time_minutes'] - fl['minimum_turn_minutes']\n\n# day of week, departure hour\nfl['day_of_week'] = fl[sched_col].dt.dayofweek\nfl['departure_hour'] = fl[sched_col].dt.hour\n\n# haul type derived from scheduled_ground_time_minutes\ndef haul_type_from_ground(x):\n    if pd.isna(x): return np.nan\n    if x < 60: return 1\n    if x <= 120: return 2\n    return 3\nfl['haul_type'] = fl['scheduled_ground_time_minutes'].apply(haul_type_from_ground)\n\n# standardizing destination airport column name (using scheduled_arrival_station_code if present)\ndest_col = next((c for c in fl.columns if 'arrival' in c.lower() and ('station' in c.lower() or 'airport' in c.lower() or 'destination' in c.lower() or 'dest' in c.lower())), None)\nif dest_col:\n    fl = fl.rename(columns={dest_col:'destination_airport_code'})\nelse:\n    fl['destination_airport_code'] = np.nan\n\nflight_core = fl[['company_id','flight_number','flight_date','departure_delay_min',\n                  'scheduled_ground_time_minutes','minimum_turn_minutes','ground_pressure',\n                  'turnaround_slack','day_of_week','departure_hour','haul_type','destination_airport_code']].copy()\nflight_core = drop_duplicated_columns(flight_core)\n\n# Bag-level features: transfer, hot-transfer, checked \nbags = pd.read_csv(BAG_FILE)\nbags['bag_type'] = bags['bag_type'].astype(str).str.strip().str.title()\n\n# Column in bag file if present\nbag_date_col = detect_date_col(bags)\nif bag_date_col:\n    bags[bag_date_col] = pd.to_datetime(bags[bag_date_col], errors='coerce').dt.date\n    bags = bags.rename(columns={bag_date_col:'flight_date'})\nelse:\n    bags['flight_date'] = np.nan\n\nbags['is_transfer'] = bags['bag_type'].str.contains('Transfer', case=False, na=False).astype(int)\nbags['is_hot_transfer'] = bags['bag_type'].str.contains('Hot Transfer', case=False, na=False).astype(int)\nbags['is_checked'] = bags['bag_type'].str.contains('Origin|Checked', case=False, na=False).astype(int)\n\nbag_group_keys = ['flight_number','flight_date']\nif 'company_id' in bags.columns:\n    bag_group_keys.insert(0,'company_id')\n\nbag_stats = bags.groupby(bag_group_keys).agg(\n    transfer_bags=('is_transfer','sum'),\n    hot_transfer_bags=('is_hot_transfer','sum'),\n    checked_bags=('is_checked','sum')\n).reset_index()\nbag_stats['total_bags'] = bag_stats['transfer_bags'] + bag_stats['checked_bags']\n\ndef safe_ratio(n,d):\n    if d > 0:\n        return n/d\n    elif d == 0 and n > 0:\n        return 5.0  # capped penalty for transfer-only flights\n    else:\n        return 0.0\n\nbag_stats['transfer_to_checked_ratio'] = bag_stats.apply(lambda r: safe_ratio(r['transfer_bags'], r['checked_bags']), axis=1)\nbag_stats['hot_transfer_ratio'] = bag_stats.apply(lambda r: safe_ratio(r['hot_transfer_bags'], r['checked_bags']), axis=1)\nbag_stats = drop_duplicated_columns(bag_stats)\n\n# Passenger loads \npnr = pd.read_csv(PNR_FILE)\npnr_date_col = detect_date_col(pnr)\nif pnr_date_col is None:\n    raise KeyError(\"Could not find a date-like column in PNR file.\")\npnr[pnr_date_col] = pd.to_datetime(pnr[pnr_date_col], errors='coerce').dt.date\npnr = pnr.rename(columns={pnr_date_col:'flight_date'})\n\nif 'total_pax' not in pnr.columns:\n    alt = next((c for c in pnr.columns if 'pax' in c.lower() or 'pass' in c.lower()), None)\n    if alt is None:\n        raise KeyError(\"PNR file missing 'total_pax' column.\")\n    pnr = pnr.rename(columns={alt:'total_pax'})\n\npnr['total_pax'] = pd.to_numeric(pnr['total_pax'], errors='coerce').fillna(0)\npax_group_keys = ['flight_number','flight_date']\nif 'company_id' in pnr.columns:\n    pax_group_keys.insert(0,'company_id')\npax_loads = pnr.groupby(pax_group_keys)['total_pax'].sum().reset_index().rename(columns={'total_pax':'pax_count'})\npax_loads = drop_duplicated_columns(pax_loads)\n\n# SSR counts \nremarks = pd.read_csv(REMARKS_FILE)\ntext_col = 'special_service_request' if 'special_service_request' in remarks.columns else next((c for c in remarks.columns if any(k in c.lower() for k in ['remark','text','note','comment'])), None)\nif text_col is None:\n    raise KeyError(\"No remark/text column found in remarks file.\")\nremarks[text_col] = remarks[text_col].astype(str).str.lower()\n\nssr_keywords = ['wheel','airport wheelchair','manual wheelchair','unaccompanied minor','unaccompanied','umnr','stroller','medical','assist']\nremarks['is_ssr'] = remarks[text_col].apply(lambda s: int(any(k in s for k in ssr_keywords)))\n\nrem_date_col = detect_date_col(remarks)\nif rem_date_col:\n    remarks[rem_date_col] = pd.to_datetime(remarks[rem_date_col], errors='coerce').dt.date\n    remarks = remarks.rename(columns={rem_date_col:'flight_date'})\nelse:\n    remarks['flight_date'] = np.nan\n\nssr_keys = ['flight_number','flight_date']\nif 'company_id' in remarks.columns:\n    ssr_keys.insert(0,'company_id')\nssr_counts = remarks.groupby(ssr_keys)['is_ssr'].sum().reset_index().rename(columns={'is_ssr':'ssr_count'})\nssr_counts = drop_duplicated_columns(ssr_counts)\n\n#  Destination congestion index (avg departure delay by destination) \nif 'destination_airport_code' in fl.columns:\n    dc = fl.groupby('destination_airport_code')['departure_delay_min'].mean().reset_index().rename(columns={'departure_delay_min':'mean_delay'})\n    dc['dest_congestion'] = minmax_scale(dc['mean_delay'])\n    dest_congestion = dc[['destination_airport_code','dest_congestion']].copy()\nelse:\n    dest_congestion = pd.DataFrame(columns=['destination_airport_code','dest_congestion'])\n\ndest_congestion = drop_duplicated_columns(dest_congestion)\n\n# ---------- 6) Merge sources into master ----------\nmaster = pax_loads.copy()\n\nmerge_b_keys = ['company_id','flight_number','flight_date'] if ('company_id' in bag_stats.columns and 'company_id' in master.columns) else ['flight_number','flight_date']\nmaster = master.merge(bag_stats, how='left', on=merge_b_keys)\nmaster = drop_duplicated_columns(master)\n\nmerge_f_keys = ['company_id','flight_number','flight_date'] if ('company_id' in flight_core.columns and 'company_id' in master.columns) else ['flight_number','flight_date']\nmaster = master.merge(flight_core, how='left', on=merge_f_keys)\nmaster = drop_duplicated_columns(master)\n\nmerge_s_keys = ['company_id','flight_number','flight_date'] if ('company_id' in ssr_counts.columns and 'company_id' in master.columns) else ['flight_number','flight_date']\nmaster = master.merge(ssr_counts, how='left', on=merge_s_keys)\nmaster = drop_duplicated_columns(master)\n\nif not dest_congestion.empty and 'destination_airport_code' in master.columns:\n    master = master.merge(dest_congestion, how='left', on='destination_airport_code')\nelse:\n    master['dest_congestion'] = 0.0\n\nmaster = drop_duplicated_columns(master)\n\n# ---------- 7) Clean/fill numeric columns and derive final features ----------\nfor c in ['transfer_bags','hot_transfer_bags','checked_bags','total_bags','transfer_to_checked_ratio','hot_transfer_ratio']:\n    if c in master.columns:\n        master[c] = pd.to_numeric(master[c], errors='coerce').fillna(0)\n\nmaster['pax_count'] = pd.to_numeric(master.get('pax_count', 0)).fillna(0)\nmaster['departure_delay_min'] = pd.to_numeric(master.get('departure_delay_min', 0)).fillna(0)\nmaster['ground_pressure'] = pd.to_numeric(master.get('ground_pressure', np.nan))\nmaster['turnaround_slack'] = pd.to_numeric(master.get('turnaround_slack', 0)).fillna(0)\nmaster['ssr_count'] = pd.to_numeric(master.get('ssr_count', 0)).fillna(0)\nmaster['day_of_week'] = pd.to_numeric(master.get('day_of_week', master['flight_date'].apply(lambda d: pd.to_datetime(d).dayofweek if pd.notna(d) else 0))).fillna(0)\nmaster['haul_type'] = pd.to_numeric(master.get('haul_type', 1)).fillna(1)\nmaster['dest_congestion'] = pd.to_numeric(master.get('dest_congestion', 0)).fillna(0)\n\nmaster['total_bags'] = master.get('total_bags', 0).fillna(0)\nmaster['bags_per_pax'] = np.where(master['pax_count']>0, master['total_bags'] / master['pax_count'], 0.0)\nmaster['ssr_rate'] = np.where(master['pax_count']>0, (master['ssr_count'] / master['pax_count']) * 100.0, 0.0)\nmaster['transfer_to_checked_ratio'] = master.get('transfer_to_checked_ratio', 0).fillna(0)\nmaster['hot_transfer_ratio'] = master.get('hot_transfer_ratio', 0).fillna(0)\n\n# Feature mapping and weights (11 variables/metrics) \nfeat_map = {\n    'departure_delay_min': ('delay_norm', 1.0),\n    'ground_pressure': ('ground_norm', 1.0),\n    'turnaround_slack': ('slack_norm', 1.0),\n    'pax_count': ('pax_norm', 1.0),\n    'transfer_to_checked_ratio': ('bagratio_norm', 1.0),\n    'hot_transfer_ratio': ('hotratio_norm', 1.5),\n    'bags_per_pax': ('bagsperpax_norm', 0.5),\n    'ssr_rate': ('ssr_norm', 1.0),\n    'day_of_week': ('dow_norm', 0.5),\n    'haul_type': ('haul_norm', 0.5),\n    'dest_congestion': ('dest_norm', 0.75)\n}\n\n# Normalizing (0-1). invert slack (lower slack -> harder) \nfor col, (outcol, weight) in feat_map.items():\n    if col not in master.columns:\n        master[col] = 0.0\n    lo, hi = safe_minmax_ser(master[col])\n    scaled = minmax_scale(master[col], lo, hi).fillna(0.0)\n    if col == 'turnaround_slack':\n        scaled = 1.0 - scaled\n    master[outcol] = scaled\n\n# Composite weighted sum \nmaster['raw_fds'] = 0.0\nfor col, (outcol, weight) in feat_map.items():\n    master['raw_fds'] += master[outcol] * weight\n\nmaster['fds'] = minmax_scale(master['raw_fds'])\n\n# Daily ranking (highest FDS = most difficult) & classification into Easy/Medium/Difficult\ndef classify_daily(df):\n    df = df.copy()\n    if df['fds'].nunique() == 1:\n        df['fds_category'] = 'Medium'\n        df['fds_rank'] = 1\n        return df\n    q1 = df['fds'].quantile(0.25)\n    q3 = df['fds'].quantile(0.75)\n    df['fds_category'] = df['fds'].apply(lambda x: 'Difficult' if x >= q3 else ('Easy' if x <= q1 else 'Medium'))\n    df['fds_rank'] = df['fds'].rank(ascending=False, method='dense').astype(int)\n    return df\n\nmaster = master.groupby('flight_date', group_keys=False).apply(classify_daily).reset_index(drop=True)\n\n# Save and preview \noutput_cols = [\n    'company_id','flight_number','flight_date','pax_count',\n    'departure_delay_min','ground_pressure','turnaround_slack','transfer_to_checked_ratio','hot_transfer_ratio',\n    'bags_per_pax','ssr_rate','day_of_week','haul_type','dest_congestion',\n    'raw_fds','fds','fds_rank','fds_category'\n]\nout = master[[c for c in output_cols if c in master.columns]].copy()\nout.to_csv(OUTPUT_FILE, index=False)\nprint(\"Saved final FDS with daily rank & category to:\", OUTPUT_FILE)\n\n# sample top difficult flights\nprint(\"\\nTop 10 (by FDS) sample:\")\nprint(out.sort_values(['flight_date','fds'], ascending=[True, False]).groupby('flight_date').head(1).head(10).to_string(index=False))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "<ipython-input-1-53ab13a6c9cb>:282: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  master = master.groupby('flight_date', group_keys=False).apply(classify_daily).reset_index(drop=True)\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Saved final FDS with daily rank & category to: Flight_Difficulty_Score_final_with_rank_and_classification.csv\n\nTop 10 (by FDS) sample:\ncompany_id  flight_number flight_date  pax_count  departure_delay_min  ground_pressure  turnaround_slack  transfer_to_checked_ratio  hot_transfer_ratio  bags_per_pax  ssr_rate  day_of_week  haul_type  dest_congestion  raw_fds      fds  fds_rank fds_category\n        UA            944  2025-08-01        399                 90.0         0.620000              95.0                        0.0                 0.0      0.000000  0.000000          4.0        3.0         0.398675 2.965966 0.568285         1    Difficult\n        UA           1599  2025-08-02        530                  6.0         0.500000              93.0                        0.0                 0.0      0.000000  0.000000          5.0        3.0         0.160901 3.035491 0.588250         1    Difficult\n        UA            845  2025-08-03        370                  7.0         0.775000              45.0                        0.0                 0.0      0.000000  0.270270          6.0        3.0         0.501522 3.089446 0.603744         1    Difficult\n        UA            845  2025-08-04        362                 28.0         0.402597             230.0                        0.0                 0.0      0.000000  0.552486          0.0        3.0         0.501522 2.594520 0.461619         1    Difficult\n        UA            972  2025-08-05        398                  8.0         0.231343             515.0                        0.0                 0.0      0.000000  0.502513          1.0        3.0         0.332415 2.586660 0.459362         1    Difficult\n        UA            987  2025-08-06        406                185.0         0.534483             135.0                        0.0                 0.0      0.000000  0.000000          2.0        3.0         0.412730 2.912403 0.552904         1    Difficult\n        UA           1387  2025-08-07        426                276.0         0.126087             603.0                        0.0                 0.0      0.000000  0.000000          3.0        3.0         0.188783 2.933356 0.558921         1    Difficult\n        UA           2427  2025-08-08        127                 11.0         0.544304              36.0                       61.0                16.0      0.488189  0.000000          4.0        2.0         0.107307 4.469337 1.000000         1    Difficult\n        UA            907  2025-08-09        376                137.0         0.837838              30.0                        0.0                 0.0      0.000000  0.000000          5.0        3.0         0.398675 3.056685 0.594336         1    Difficult\n        UA            907  2025-08-10        305                304.0         1.240000             -30.0                        0.0                 0.0      0.000000  0.000000          6.0        3.0         0.398675 3.175548 0.628469         1    Difficult\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "d39cfc89-8c43-44da-af20-fac1da11d00a",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}